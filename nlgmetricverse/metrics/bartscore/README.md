# Metric Card for BartScore

## Metric Description
BARTScore formulates evaluating generated text as a text generation task from pre-trained seq2seq models. It operationalizes this idea using BART, an encoder-decoder based pre-trained language model. BARTScore is conceptually simple and empirically effective, directly evaluating text through the lens of its probability of being generated from or generating other textual inputs and outputs. BARTScore has three main advantages.<br>
- It allows to fully take advantage of the parameters learned during the pre-training phase, without requiring extra architectural parameters or human judgments, i.e., parameter- and data-efficient.<br>
- BARTScore can better support evaluation of generated text from seven different perspectives (informativeness, relevance, fluency, coherence, factuality, semantic coverage, adequacy) by adjusting the inputs and outputs of the conditional text generation problem. This is in contrast to most previous work, which mostly examines correlation of the devised metrics with output quality from a limited number of perspectives.<br>
- BARTScore can be further enhanced by providing textual prompts or updating the underlying model by fine-tuning BART based on downstream generation tasks (e.g., text summarization). BARTScore achieves the best performance on 16 of 22 settings against existing top-scoring metric.<br>
<img src="https://render.githubusercontent.com/render/math?math={BARTScore = \sum_{t=1}^m w_t log p(y_t|y_{<t},x,\theta)}##gh-light-mode-only"><br>
where <img src="https://render.githubusercontent.com/render/math?math={\theta}##gh-light-mode-only"> are model parameters.

The authors present four methods for using BARTScore based on different generation directions.<br>
- <b>Faithfulness</b> (<img src="https://render.githubusercontent.com/render/math?math={s \rightarrow h}##gh-light-mode-only">): from source document to hypothesis <img src="https://render.githubusercontent.com/render/math?math={p(h|s, \theta)}##gh-light-mode-only">. This direction measures how likely it is that the hypothesis could be generated based on the source text (factuality, relevance). This measure can also be used for estimating measures of the quality of only the target text (coherence, fluency).<br>
- <b>Precision</b> (<img src="https://render.githubusercontent.com/render/math?math={r \rightarrow h}##gh-light-mode-only">): from reference text to system-generated text <img src="https://render.githubusercontent.com/render/math?math={p(h|r, \theta)}##gh-light-mode-only">. This direction assesses how likely the hypothesis could be constructed based on the gold reference and is suitable for the precision-focused scenario.<br>
- <b>Recall</b> (<img src="https://render.githubusercontent.com/render/math?math={h \rightarrow r}##gh-light-mode-only">): from system-generated text to reference text <img src="https://render.githubusercontent.com/render/math?math={p(r|h, \theta)}##gh-light-mode-only">. This version quantifies how easily a gold reference could be generated by the hypothesis and is suitable for pyramid-based evaluation (semantic coverage).<br>
- <b>F-score</b> (<img src="https://render.githubusercontent.com/render/math?math={r \leftrightarrow h}##gh-light-mode-only">): consider both directions and use the arithmetic average of Precision and Recall ones. This version can be broadly used to evaluate the semantic overlap (informativeness, adequacy) between reference texts and generated texts.

### Inputs
- **predictions** (`list`): list of predictions to score. Each prediction
        should be a string with tokens separated by spaces.
- **references** (`list`): list of reference for each prediction. Each
        reference should be a string with tokens separated by spaces.
- **rouge_types** (`list`): A list of rouge types to calculate. Defaults to `['rouge1', 'rouge2', 'rougeL', 'rougeLsum']`.
    - Valid rouge types:
        - `"rouge1"`: unigram (1-gram) based scoring
        - `"rouge2"`: bigram (2-gram) based scoring
        - `"rougeL"`: Longest common subsequence based scoring.
        - `"rougeLSum"`: splits text using `"\n"`
        - See [here](https://github.com/huggingface/datasets/issues/617) for more information
- **use_aggregator** (`boolean`): If True, returns aggregates. Defaults to `True`.
- **use_stemmer** (`boolean`): If `True`, uses Porter stemmer to strip word suffixes. Defaults to `False`.


## Bounds
<img src="https://render.githubusercontent.com/render/math?math={]-inf,0[}##gh-light-mode-only"><br>
Since BARTScore uses the average log-likelihood for target tokens, the calculated scores will be smaller than 0 (the probability is between 0 and 1, so the log of it should be negative). The higher the
log-likelihood, the higher the probability. To give an example, if SummaryA gets a score of -1 while SummaryB gets a score of -100, this means that the model thinks SummaryA is better than summaryB.

## Citation
```bibtex
@misc{yuan2021bartscore,
      title={BARTScore: Evaluating Generated Text as Text Generation}, 
      author={Weizhe Yuan and Graham Neubig and Pengfei Liu},
      year={2021},
      eprint={2106.11520},
      archivePrefix={arXiv},
      primaryClass={cs.CL}
}
```

## Further References
- See the `README.md` file at [https://github.com/neulab/BARTScore](https://github.com/neulab/BARTScore) for more information.
