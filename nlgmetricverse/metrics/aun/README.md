# Metric Card for Average Unique N-grams

## Metric Description
Summarization is the task of shortening a given document(s) while maintaining the most important information. In general, a good summarizer should generate a summary that is syntactically accurate, semantically correct, coherent, and non-redundant. While extractive methods tend to have better performance on the first two aspects, they are typically less coherent and more redundant than abstractive ones, where new sentences are often generated by sentence fu- sion and compression, which helps detecting and removing redundancy.

The following metric measures redundancy in the source documents and in the generated summaries.

**Unique n-gram ratio**: proposed in Peyrard et al. (2017), it measures n-grams uniqueness; the lower it is, the more redundant the document is.

$Uniq\_ngram\_ratio=\frac{count(uniq\_ngram)}{count(ngram)}$

When we compare the redundancy of long vs. short documents with respect to this metric on four popular datasets for summarization, we observe that long documents are sub- stantially more redundant than short ones.

# Inputs
-  **predictions** (`list`): An instance of EvaluationInstance containing the predicted text.
-  **references** (`list`): An instance of EvaluationInstance containing the reference text.
-  **n** (int): The size of the n-grams to use for computing abstractness. Defaults to 1 (unigrams).

# Outputs
-  **aun** (`float`): The average unique n-gram ratio.

# Examples
```python
predictions = ["Peace in the dormitory, peace in the world.", "There is a cat on the mat."]
references = ["Peace at home, peace in th world.", "The cat is playing on the mat."]

scorer = NLGMetricverse(metrics=load_metric("accuracy"))
scores = scorer(predictions=predictions, references=references, reduce_fn=REDUCTION_FUNCTION)
print(json.dumps(scores, indent=4))
{
    "total_items": 2,
    "empty_items": 0,
    "total_time_elapsed": 0.007993459701538086,
    "aun": {
        "score": 0.9310344827586207,
        "time_elapsed": 0.007993459701538086
    }
}
```

## Citation(s)
```bibtex
@inproceedings{xiao-carenini-2020-systematically,
    title = "Systematically Exploring Redundancy Reduction in Summarizing Long Documents",
    author = "Xiao, Wen  and
      Carenini, Giuseppe",
    booktitle = "Proceedings of the 1st Conference of the Asia-Pacific Chapter of the Association for Computational Linguistics and the 10th International Joint Conference on Natural Language Processing",
    month = dec,
    year = "2020",
    address = "Suzhou, China",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2020.aacl-main.51",
    pages = "516--528",
    abstract = "Our analysis of large summarization datasets indicates that redundancy is a very serious problem when summarizing long documents. Yet, redundancy reduction has not been thoroughly investigated in neural summarization. In this work, we systematically explore and compare different ways to deal with redundancy when summarizing long documents. Specifically, we organize existing methods into categories based on when and how the redundancy is considered. Then, in the context of these categories, we propose three additional methods balancing non-redundancy and importance in a general and flexible way. In a series of experiments, we show that our proposed methods achieve the state-of-the-art with respect to ROUGE scores on two scientific paper datasets, Pubmed and arXiv, while reducing redundancy significantly.",
}
```