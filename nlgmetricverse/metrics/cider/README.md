# Metric Card for CIDer

## Metric Description
Recent advances in object recognition, attribute classification, action classification and crowd- sourcing have increased the interest in solving higher level scene understanding problems. One such problem is generating human-like descriptions of an image. In spite of the growing interest in this area, the evaluation of novel sentences generated by automatic approaches remains chal- lenging. Evaluation is critical for measuring progress and spurring improvements in the state of the art. This has already been shown in various problems in computer vi- sion, such as detection, segmentation, and stereo.

The goal is to automatically evaluate for image $I_i$ how well a candidate sentence $c_i$ matches the consensus of a set of image descriptions $S_i = \{s_{i1} , . . . , s_{im}\}$. All words in the sentences (both candidate and references) are first mapped to their stem or root forms. That is, “fishes”, “fish- ing” and “fished” all get reduced to “fish.”


$CIDEr_n$ score for n-grams of length $n$ is computed using the average cosine similarity between the candidate sentence $c_i$ and the reference sentences $s_{ij}$, which accounts for both precision and recall:

$CIDer_n(c_i, S_i) = \frac{1}{m} \sum_j{\frac{
    g^n(c_i) \cdot g^n(s_{ij})
}{
    ||G^n(c_i)||\cdot||g^n(s_{ij})||
}}$

where $g^n(c_i)$ is a vector formed by $g_k(c_i)$ (the TF-IDF weighting) corresponding to all n-grams of length $n$ and $||g^n(c_i)||$ is the magnitude of the vector $g^n(c_i)$. Similarity for $g^n(s_{ij})$.

## Citation(s)
```bibtex
@inproceedings{DBLP:conf/cvpr/VedantamZP15,
  author    = {Ramakrishna Vedantam and
               C. Lawrence Zitnick and
               Devi Parikh},
  title     = {CIDEr: Consensus-based image description evaluation},
  booktitle = {{CVPR}},
  pages     = {4566--4575},
  publisher = {{IEEE} Computer Society},
  year      = {2015}
}
```

